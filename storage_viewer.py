#!/usr/bin/env python3
"""
æ•°æ®å­˜å‚¨æŸ¥çœ‹å™¨ - å¯è§†åŒ–ç³»ç»Ÿå­˜å‚¨çš„æ•°æ®
"""
import os
import json
import pickle
from pathlib import Path
from typing import Dict, Any, List
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StorageViewer:
    """æ•°æ®å­˜å‚¨æŸ¥çœ‹å™¨"""
    
    def __init__(self, base_path: str = "F:\\AI_langchain"):
        self.base_path = Path(base_path)
        self.cache_dir = self.base_path / "cache"
        self.docs_dir = self.base_path / "docs"
        self.uploads_dir = self.base_path / "uploads"
    
    def view_cache_data(self) -> Dict[str, Any]:
        """æŸ¥çœ‹ç¼“å­˜æ•°æ®"""
        cache_info = {
            "text_cache": {},
            "vector_cache": {},
            "metadata_cache": {},
            "total_size_mb": 0
        }
        
        # æ£€æŸ¥æ–‡æœ¬ç¼“å­˜
        text_dir = self.cache_dir / "text"
        if text_dir.exists():
            cache_info["text_cache"] = {
                "files": [],
                "count": 0,
                "size_mb": 0
            }
            
            for cache_file in text_dir.glob("*.cache"):
                try:
                    with open(cache_file, 'rb') as f:
                        data = pickle.load(f)
                    
                    cache_info["text_cache"]["files"].append({
                        "filename": cache_file.name,
                        "size_bytes": cache_file.stat().st_size,
                        "timestamp": data.get('timestamp', 'unknown'),
                        "ttl": data.get('ttl', 'unknown'),
                        "preview": str(data.get('value', ''))[:100] + "..."
                    })
                    cache_info["text_cache"]["size_mb"] += cache_file.stat().st_size / 1024 / 1024
                    
                except Exception as e:
                    logger.warning(f"æ— æ³•è¯»å–ç¼“å­˜æ–‡ä»¶ {cache_file}: {e}")
            
            cache_info["text_cache"]["count"] = len(cache_info["text_cache"]["files"])
        
        # æ£€æŸ¥å‘é‡ç¼“å­˜
        vector_dir = self.cache_dir / "vector"
        if vector_dir.exists():
            cache_info["vector_cache"] = {
                "files": [],
                "count": 0,
                "size_mb": 0
            }
            
            for vector_file in vector_dir.glob("*"):
                cache_info["vector_cache"]["files"].append({
                    "filename": vector_file.name,
                    "size_bytes": vector_file.stat().st_size,
                    "type": "file" if vector_file.is_file() else "directory"
                })
                cache_info["vector_cache"]["size_mb"] += vector_file.stat().st_size / 1024 / 1024
            
            cache_info["vector_cache"]["count"] = len(cache_info["vector_cache"]["files"])
        
        # è®¡ç®—æ€»å¤§å°
        cache_info["total_size_mb"] = cache_info["text_cache"].get("size_mb", 0) + cache_info["vector_cache"].get("size_mb", 0)
        
        return cache_info
    
    def view_document_files(self) -> Dict[str, Any]:
        """æŸ¥çœ‹æ–‡æ¡£æ–‡ä»¶"""
        doc_info = {
            "docs_folder": [],
            "uploads_folder": [],
            "total_pdfs": 0,
            "total_size_mb": 0
        }
        
        # æ£€æŸ¥docsæ–‡ä»¶å¤¹
        if self.docs_dir.exists():
            for pdf_file in self.docs_dir.glob("*.pdf"):
                doc_info["docs_folder"].append({
                    "filename": pdf_file.name,
                    "size_bytes": pdf_file.stat().st_size,
                    "size_mb": round(pdf_file.stat().st_size / 1024 / 1024, 2),
                    "path": str(pdf_file)
                })
                doc_info["total_size_mb"] += pdf_file.stat().st_size / 1024 / 1024
        
        # æ£€æŸ¥uploadsæ–‡ä»¶å¤¹
        if self.uploads_dir.exists():
            for pdf_file in self.uploads_dir.glob("*.pdf"):
                doc_info["uploads_folder"].append({
                    "filename": pdf_file.name,
                    "size_bytes": pdf_file.stat().st_size,
                    "size_mb": round(pdf_file.stat().st_size / 1024 / 1024, 2),
                    "path": str(pdf_file)
                })
                doc_info["total_size_mb"] += pdf_file.stat().st_size / 1024 / 1024
        
        doc_info["total_pdfs"] = len(doc_info["docs_folder"]) + len(doc_info["uploads_folder"])
        
        return doc_info
    
    def view_chat_sessions(self) -> Dict[str, Any]:
        """æŸ¥çœ‹èŠå¤©è®°å½•"""
        try:
            # å¯¼å…¥chat_manager
            import sys
            sys.path.append(str(self.base_path))
            from src.core.chat_manager import ChatManager
            
            chat_manager = ChatManager()
            sessions = chat_manager.get_all_sessions()
            
            return {
                "total_sessions": len(sessions),
                "sessions": sessions,
                "session_files": list((self.base_path / "cache" / "metadata").glob("*.json")) if (self.base_path / "cache" / "metadata").exists() else []
            }
        except Exception as e:
            logger.error(f"æ— æ³•åŠ è½½èŠå¤©è®°å½•: {e}")
            return {"total_sessions": 0, "sessions": [], "error": str(e)}
    
    def generate_report(self) -> str:
        """ç”Ÿæˆå®Œæ•´çš„å­˜å‚¨æŠ¥å‘Š"""
        cache_data = self.view_cache_data()
        doc_data = self.view_document_files()
        chat_data = self.view_chat_sessions()
        
        report = f"""
ğŸ“Š AIæ–‡æ¡£é—®ç­”ç³»ç»Ÿ - æ•°æ®å­˜å‚¨æŠ¥å‘Š
{'='*50}

ğŸ“ å­˜å‚¨ä½ç½®:
   åŸºç¡€ç›®å½•: {self.base_path}
   ç¼“å­˜ç›®å½•: {self.cache_dir}
   æ–‡æ¡£ç›®å½•: {self.docs_dir}
   ä¸Šä¼ ç›®å½•: {self.uploads_dir}

ğŸ’¾ ç¼“å­˜æ•°æ®:
   æ–‡æœ¬ç¼“å­˜æ–‡ä»¶: {cache_data['text_cache'].get('count', 0)} ä¸ª
   å‘é‡ç¼“å­˜æ–‡ä»¶: {cache_data['vector_cache'].get('count', 0)} ä¸ª
   æ€»ç¼“å­˜å¤§å°: {round(cache_data['total_size_mb'], 2)} MB

ğŸ“„ æ–‡æ¡£æ–‡ä»¶:
   PDFæ–‡ä»¶æ€»æ•°: {doc_data['total_pdfs']} ä¸ª
   æ€»æ–‡ä»¶å¤§å°: {round(doc_data['total_size_mb'], 2)} MB
   
   ğŸ“‚ docsæ–‡ä»¶å¤¹:
{chr(10).join([f"      - {f['filename']} ({f['size_mb']} MB)" for f in doc_data['docs_folder']]) or "      (ç©º)"}
   
   ğŸ“‚ uploadsæ–‡ä»¶å¤¹:
{chr(10).join([f"      - {f['filename']} ({f['size_mb']} MB)" for f in doc_data['uploads_folder']]) or "      (ç©º)"}

ğŸ’¬ èŠå¤©è®°å½•:
   æ€»ä¼šè¯æ•°: {chat_data['total_sessions']}
   ä¼šè¯æ–‡ä»¶: {len(chat_data['session_files'])} ä¸ª

ğŸ” æŸ¥çœ‹æ–¹æ³•:
   1. ç›´æ¥æŸ¥çœ‹æ–‡ä»¶å¤¹: {self.cache_dir}
   2. ä½¿ç”¨Pythonè„šæœ¬: python storage_viewer.py
   3. é€šè¿‡Webç•Œé¢: åœ¨ç³»ç»Ÿå¯åŠ¨åæŸ¥çœ‹"ç³»ç»ŸçŠ¶æ€"æ ‡ç­¾é¡µ

"""
        return report

def main():
    """ä¸»å‡½æ•°"""
    viewer = StorageViewer()
    print(viewer.generate_report())
    
    # è¯¦ç»†æŸ¥çœ‹ç¼“å­˜å†…å®¹
    print("\nğŸ“‹ ç¼“å­˜å†…å®¹è¯¦æƒ…:")
    cache_data = viewer.view_cache_data()
    
    if cache_data['text_cache'].get('files'):
        print("\næ–‡æœ¬ç¼“å­˜:")
        for file_info in cache_data['text_cache']['files'][:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            print(f"  ğŸ“„ {file_info['filename']}")
            print(f"     æ—¶é—´: {file_info['timestamp']}")
            print(f"     é¢„è§ˆ: {file_info['preview']}")
            print()

if __name__ == "__main__":
    main()